
# GNN architecture overview 

To perform interpretable and convenient drug-target interaction predictions using knowledge graph, we have implemented a graph neural network to make predictions, however, we use an unconventional data encoding to mitigate the need to do drug-protein embedding comparisons. 

## Premise 

We rationalize that in most biological knowledge graphs, rational drug-target predictions should be made based on the unique path(s) from drug->target. This is somewhat different than traditional GNN prediction tasks that typically recognize local structure or patterns predictive of the outcome.

## Data encoding 

All nodes in the input graph are encoded with a zero input value, except for the drug we wish to predict targets for (designated `src`), we then use Graph convolutions to create a node embedding, which we rationalize is analagous to a diffusion process from the nonzero `src` node to all potential protein targets. To further encourage this diffusion-like process, we use node-level normalization schemes and prevent bias in the convolutions.

The result is a drug-specific protein embedding, generated by a diffusion-like process (modeled by graph convolutions) from drug->protein. This approach removes the need to make link predictions using both source and target node embeddings, and formulates it as a node-prediction problem. 

## Graph augmentation during training 

Importantly, the knowledge graph includes all training DTIs, and therefore every training observation has a direct and simple route from drug->protein, which is likely to make prediction trivial and non-generalizable to unseen DTIs. To ensure that our GNN model learns to predict based on alternative routes in the KG, we remove each observation training DTI. Notably, this works well for generalization to unseen DTIs, however, at evaluation time predicting training DTIs (without edge removal) typically results in a zero probability prediction.

# Predicting new DTIs 

In this framework, to predict all new DTIs requires running `N_drugs` forward passes (assuming unbatched), where each forward passes computes all the predicted DTIs from a single given node. This is tractable and convenient, however, as we mentioned in `Graph augmentation during training`, training edges that are present in the knowledge graph will result in near zero edge probabilities and therefore all training edges will appear negative. To accurately infer training partition edges requires removing the training edge from the knowledge graph prior to predicting. In practice, this is not necessary as we usually do not care about characterizing known edge probabilities. 

## Interpretation 

Using previously established methods such as `GNNExplainer` we can identify the links that are critically involved in the prediction of a given observation.

NOTE: For some reason GATv2 does not work well with GNNExplainer. 

## Knowledge Distillation 

We can also use other less readily intepretable methods, such as Complex^2, to predict DTIs that can then be used to train a GNN model which can be explained.

## Memory footprint and scalability 

While we can conveniently batch observations, it does require unique knowledge graphs for each obs and therefore consumes significant memory during training. In practice, this limits the batch size to less than 10. Additionally, using large models (e.g., GAT with channels > 64) is usually prohibitively expensive (>24GB VRAM). 

## Future directions 

- Currently every triple (drug-target link) is treated as a unique observation, however, this could be improved by aggregating all DTIs that involve a given drug by setting a multiple nonzero targets (`y` values). It is unclear how much of an impact this would have on training as there are only ~1000 observations and ~700 drugs, suggesting that most drugs have only 1-2 known drug targets. 

## Note on evaluation during training 

During training we evaluate performance on the validation set, however, we do not remove the training or test dtis, and therefore the performance metrics will be slightly lower than the true performance behaviors. We evaluate the true metrics on the test set after training. 
