###############################################################################
# AML-TKG configuration for KG construction (01 -> 09d)
#
# Shared roots
# - dirs.data_root:    all input data under this directory
# - dirs.out_root:     experiment outputs will be placed under out_root/{experiment.id}
# - experiment.id:     unique identifier; creates unique output directories per config
# - experiment.seed:   default random seed for scripts (can be overridden per-step)
#
# Shared model/embedding config used by multiple steps
# - shared.prot_model_name / smiles_model_name etc. used in 09a; optional parity with 02/08a
###############################################################################

dirs:
  data_root: "/home/exacloud/gscratch/mcweeney_lab/evans/data/"     # e.g., /home/.../TKG-DTI/data
  out_root:  "/home/exacloud/gscratch/mcweeney_lab/evans/outputs_/TKGDTI_workflow"     # e.g., /home/.../TKG-DTI/runs
  script_dir: "/home/exacloud/gscratch/mcweeney_lab/evans/TKG-DTI/workflow/scripts"    # absolute path to workflow scripts

experiment:
  id: "aml-tkg-01"
  seed: 0

shared:
  # protein embedding (ProtBert)
  prot_model_name: facebook/esm2_t30_150M_UR50D  #"Rostlab/prot_bert" # facebook/esm2_t33_650M_UR50D
  prot_repr: "mean"
  prot_batch_size: 64
  prot_max_len: 1024

  # drug embedding (ChemBERTa)
  smiles_model_name: yzimmermann/ChemBERTa-zinc-base-v1-safetensors #"yzimmermann/ChemBERTa-77M-MLM-safetensors" # 
  smiles_repr: "mean"
  smiles_batch_size: 128
  smiles_max_len: 1024

###############################################################################
# Per-script configuration (arguments map to the corresponding script flags)
###############################################################################

# 01: kg_construction_01__drug_interacts_protein.py
step_01:
  seed: 0
  assay_types: ["Kd", "Ki", "IC50", "EC50"]         # options: Kd, Ki, IC50, EC50
  max_assay_value: 1000               # nM threshold
  TIERS: ["TIER_1"]                   # options: TIER_1..TIER_5*

# 02: kg_construction_02__drug_similar_drug.py
step_02:
  seed: 0
  method: "threshold"                       # options: threshold, knn
  q_threshold: 0.95
  knn_k: 3

# 03: kg_construction_03__drug_associates_disease.py
step_03:
  seed: 0

# 04: kg_construction_04__protein_associates_disease.py
step_04:
  seed: 0

# 05: kg_construction_05__protein_interacts_protein.py
step_05:
  seed: 0

# 06: kg_construction_06__protein_isin_pathway.py
step_06:
  seed: 0

# 07: kg_construction_07__lincs_drug_perturbed_expression.py
step_07:
  seed: 0
  score_threshold: 5                   # 0-10
  sig_sign: "-"                        # options: '-', '+', '+/-'

# 08a: kg_construction_08a__embed_amino_acids.py
step_08a:
  seed: 0

# 08b: kg_construction_08b__protein_protein_similarity.py
step_08b:
  seed: 0
  sim_quantile: 0.999

# 09a: kg_construction_09a__binding_affinity_make.py
step_09a:
  seed: 0
  train_split: "train[:90%]"
  test_split: "train[90%:]"

# 09b: kg_construction_09b__binding_affinity_train.py
step_09b:
  seed: 1
  nsamples: 100
  batch_size: 10000
  lr: 1e-4
  num_epochs: 25
  hidden_channels: 256
  layers: 2
  stochastic_channels: 32
  hnet_width: 128                          # hypernet width
  p_val: 0.5
  wd: 0.0                                  # weight decay
  norm: "layer"                            # normalization type ['none', 'layer'] # NOTE: batch doesn't work with hypernet
  nonlin: "elu"                            # nonlinearity type
  dropout: 0.0                             # dropout rate
  learn_pz: true                          # learn pz boolean flag

# 09c: kg_construction_09c__binding_affinity_eval.py
step_09c:
  samples: 200
  batch_size: 10000
  device: "auto"                       # options: auto, cpu, cuda

# 09d: kg_construction_09d__predicted_binding_affinity.py
step_09d:
  q_weak: 0.05
  q_strong: 0.95

# 10: kg_construction_10_make_tkg.py
step_10:
  seed: 0
  k_folds: 5                             # Number of K-fold cross-validation splits
  val_prop: 0.05                         # Proportion of training data for validation
  tkg_output_dir: "tkge_no_patient"       # Output directory name for TKG data
  relations_root: "relations"             # Root directory containing relation CSV files  
  exclude_patient_relations: true         # Exclude relations involving patient data
  no_rev: false                           # Exclude reverse relations (_rev files)

# 11: train_complex2.py
step_11:
  optim: "adam"                          # optimizer [adam, adagrad, adan]
  wd: 0                                  # weight decay
  channels: 512                         # number of hidden channels
  batch_size: 10000                      # batch size
  n_epochs: 100                          # number of training epochs
  num_workers: 12                        # number of data loader workers
  lr: 0.01                               # learning rate
  dropout: 0.0                           # dropout rate
  lr_scheduler: false                    # whether to use lr scheduler (reduce on plateau)
  verbose: false                         # verbosity
  log_every: 1                           # how often to log validation performance during training
  target_relations: null                 # target relations to use for loss scaling
  patience: 10                           # early stopping patience; log_every*patience epochs without improvement
  use_cpu: false                         # use cpu instead of gpu
  target_relation: "drug,targets,gene"   # tuple key of target relation
  target_metric: "mrr"                   # metric to use for early stopping [hits@10, mrr, auroc]
  remove_relation_idx: null              # index of relation to remove from knowledge graph

# 12: train_gnn.py
step_12:
  wd: 1e-7                                  # weight decay
  channels: 12                            # number of hidden channels
  layers: 4                              # number of GNN layers
  batch_size: 5                          # batch size
  n_epochs: 100                            # number of training epochs
  num_workers: 12                        # number of data loader workers
  lr: 0.001                              # learning rate
  dropout: 0.1                           # dropout rate
  verbose: false                         # verbosity
  nonlin: "mish"                          # nonlinearity to use in GNN
  heads: 2                              # number of attention heads
  bias: false                            # whether to use bias in GNN layers
  edge_dim: 6                            # dimension of edge features
  checkpoint: false                      # whether to use gradient checkpointing
  log_every: 1                           # log every n epochs
  residual: true                        # whether to use residual connections
  compile: false                         # whether to compile the model
  norm: "layer"                          # normalization to use in GNN [layer, batch, none]
  patience: 5                            # early stopping patience
  conv: "gat"                            # GNN convolution to use
  target_metric: "mrr"                   # metric to use for early stopping [hits@10, mrr, auroc]
  target_relation: "drug,targets,gene"   # tuple key of target relation
  heteroA: false                         # whether the data is from the HeteroA dataset
  remove_relation_idx: null              # index of relation to remove from knowledge graph

